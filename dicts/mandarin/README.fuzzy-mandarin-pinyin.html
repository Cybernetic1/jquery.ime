<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Fuzzy pinyin matching</title>
		<meta name='Generator' content='Zim 0.54'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-decoration: underline;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
		</style>
	</head>
	<body>

<!-- Wiki content -->

<h1>Fuzzy pinyin matching</h1>

<h4>(A)  Fuzzy matching of single characters</h4>

<p>
Each character <img src="./README.fuzzy-mandarin-pinyin/equation.png" alt="">.<br>
</p>

<p>
Misspelled character <img src="./README.fuzzy-mandarin-pinyin/equation001.png" alt="">.<br>
</p>

<p>
Distance between each pair:<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation002.png" alt=""><br>
</div>
and<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation003.png" alt=""><br>
</div>
where <em>d</em>(·,·) is the distance metric induced by the classification tree (see below).<br>
</p>

<p>
Then distance between the 2 characters <em>c, c'</em> = <br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation005.png" alt=""><br>
</div>
or we can use<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation006.png" alt=""><br>
</div>
to give more weight to the nuclei because people usually have less trouble with consonants.<br>
</p>

<p>
We can use (1 - <em>d</em>) to represent the matching score.<br>
</p>

<p>
Each character <em>c</em> also has an occurrence frequency, <em>f</em>.  The distribution of <em>f</em> follows the famous <a href="http://en.wikipedia.org/wiki/Zipf's_law" title="Zipf's Law">Zipf's Law</a>, which says that the most frequent char will occur approximately twice as often as the second most frequent char, three times as often as the third most frequent char, etc.  In other words, if we plot log(rank order) against log(frequency), we get a straight line.<br>
</p>

<p>
Now we have 2 things:  one is the frequecy or frequency ranking, the other is the distance measure for spelling errors.  We need to somehow unify the two measures.  There may be more than one way to do it.<br>
</p>

<p>
Earlier I had used a somewhat <em>ad hoc</em> formula:<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation007.png" alt=""><br>
</div>
where <em>C,D</em> are parametric constants the user can tweak.  The purpose of <em>C</em> is to make the factor positive.  If the factor <img src="./README.fuzzy-mandarin-pinyin/equation008.png" alt="">, distant spellings will have low ranking (this is suitable for more experienced users, who usually spell correctly), otherwise the influence of <em>f</em> would be bigger, more misspelled characters will be displayed instead of rare (low-frequency) ones. But this is not a very good idea, as it lacks any deep mathematical justification.<br>
</p>

<h4>Bayesian reasoning</h4>

<p>
A better approach may be Bayesian reasoning.  I noticed that the frequency can be regarded as the "prior" probability of a character occurring in any text, without further information.  We are given the "observation / data" which is the (possibly erroneous) spelling given by the user, and we want to use this observation to guess what is the user's intended character, ie our "hypothesis".<br>
</p>

<p>
An easy way to understand and remember Bayes' Law is via this basic rule in probability:<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation010.png" alt=""><br>
</div>
where (A, B) denotes "A and B" both being true.  Upon rearranging, this gives the familiar form of Bayes' Law:<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation009.png" alt="">.<br>
</div>
The profound usefulness of Bayesian reasoning comes from the fact that the roles of A and B are not symmetric as they appear in the formula.  We denote H as the hypothesis (one among many), by which we seek to explain the observation or data, D.  Then:<br>
<div style='padding-left: 30pt'>
<img src="./README.fuzzy-mandarin-pinyin/equation011.png" alt=""><br>
</div>
where P(H|D) is the probability that this particular hypothesis is true given the observation, P(H) is the prior probability of this hypothesis (our initial belief in this hypothesis without any observation), P(D) is the prior probability of getting this particular observation. Bayesian reaoning is powerful because P(H|D) is usually difficult to compute directly, while P(D|H) is usually much easier to compute.  The reason behind this is that we usually understand better the mechanism by which a hypothesis generates an observation, rather than the other way around.  That is because our minds work by using hypotheses to <em>explain</em> observations, and we have explicit knowledge of this mechanism.<br>
</p>

<p>
In our case the observation D is the spelling provided by the user.  The hypothesis H is the character intended by the user.  It is not easy to guess what is the intended character given the possibly erroneous spelling, but it is somwhat easier if we know what is the intended character, and guess the ways in which it can be misspelled — since we already know its correct spelling, and our distance metric gives how far it is from the correct form;  we can assign a conditional probability, P(D|H) = P(spelling | character), as a function of the error distance.<br>
</p>

<p>
P(D|H) may be given by (1.0 - error distance), since we have confined the distance in [0,1].  But we need to avoid the extremal probability values of 0 and 1.  (However, conditional probabilities do not need to be normalized to sum to 1.  I am still thinking on whether P(D|H) must obey further constraints...)<br>
</p>

<p>
P(H) is the prior probability of a char, this is same as the character frequency.<br>
</p>

<p>
P(D) is the prior probability of a spelling.  We can deduce this from the character frequencies, converting the characters to their correct spellings, and add up.<br>
</p>

<h4>Algorithm</h4>

<p>
We can just scan through the entire set of characters (which is on the order of 10K) with everyone mixed together, calculate the distance of each char with our input char, then calculate the new ranking.  Of course the lower ranking ones will be cut off.<br>
</p>

<h4>Distance between 2 consonants / nuclei</h4>

<p>
This is the classification tree, based on my estimation of which sounds are close to each other (Of course the reader should feel free to improve it):<br>
<img src="classification-consonants.png" alt=""><br>
</p>

<p>
A similar map has been made for distances between nuclei:<br>
<img src="classification-nuclei.png" alt=""><br>
</p>

<p>
We can assign distances this way:<br>
</p>

<p>
Each cluster may have <em>N</em> elements, and <em>e</em> is the average distance from the elements to its centroid (smaller <em>e</em> means a more compact cluster).  A sub-cluster is represented by its centroid as a single element in the parent cluster.<br>
</p>

<p>
The distance between 2 leaf elements can be taken to be the <em>e</em> value of their lowest common parent cluster.<br>
</p>

<p>
We can normalize the <em>e</em> value at the root to 1 (the biggest average distance).<br>
</p>

<h4>(B)  Fuzzy matching of multi-character words/phrases</h4>

<p>
The key here is to deal with the large number of combinatorial variations.  In our algorithm for (A) we calculated the distance between the input char and all other chars.  We need to save some time this time.<br>
</p>

<p>
Remember, the multi-char words / phrases (hereby known as "words"), have character segmentations that we already know.<br>
</p>

<p>
Our input is an unsegmented blob of letters.  We could segment them first and match them with existing words, as shown in this diagram:<br>
</p>

<p>
<img src="segmented-word-matching.png" alt=""><br>
</p>

<p>
After segmentation, the number of characters must match, so we are just matching 2 words, char-by-char.  This is already amenable to brute-force -- the number of words of a fixed length is on the order of 50K).  We could, of course, device more sophisticated algorithms to accelerate the search, but this is not necessary for our purposes.<br>
</p>

<h4>(C)  Find word / phrase by character(s)</h4>

<p>
Notice that we have done "from spelling to words" (用英文字母找詞語), but we could also do "from chars to words containing the chars" (用中文單字找詞語, as long as the characters occur in the target, in that order, but with other chars possibly interspersing).<br>
</p>

<p>
This is also amenable to brute force.<br>
</p>

<p>
==============================<br>
</p>

<p>
In conclusion, all we need is to find the biggest (and most modern) word lists.  Users may want to stay subscribed to our web site because they want to have the latest word collections.<br>
</p>


<!-- End wiki content -->

	</body>

</html>
